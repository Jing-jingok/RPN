{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inpyt image size\n",
    "ISIZE = (800, 800)\n",
    "\n",
    "#Imagenet statistics\n",
    "imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "\n",
    "## Anchor Box Specs\n",
    "ratios = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corner_rect(bb, color='red'):\n",
    "    bb = np.array(bb, dtype=np.float32)\n",
    "    return plt.Rectangle((bb[0], bb[1]), bb[2]-bb[0], bb[3]-bb[1], color=color,\n",
    "                         fill=False, lw=3)\n",
    "   \n",
    "def show_corner_bbs(im, bbs):\n",
    "    #im = np.asarray(im).astype(int).transpose(1,2,0)\n",
    "    im = np.asarray(im).transpose(1,2,0)\n",
    "    im = unnormalize(im)\n",
    "    plt.imshow(im)\n",
    "    for bb in bbs:\n",
    "        plt.gca().add_patch(create_corner_rect(bb))    \n",
    "        \n",
    "def normalize(im):\n",
    "    #im = im.astype(np.float32)/255.\n",
    "    im = im/255.\n",
    "    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]  \n",
    "\n",
    "def unnormalize(im):\n",
    "    im = im.astype(np.float32)\n",
    "    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    im = (im *imagenet_stats[1] +  imagenet_stats[0])*255.\n",
    "    im = im.astype(np.int)\n",
    "    return im\n",
    "\n",
    "def evaluate_feature_map(imgs, req_features):\n",
    "    k = imgs.clone()\n",
    "    for m in req_features:\n",
    "        k = m(k)\n",
    "    return k\n",
    "\n",
    "def plot_feature_map(k, img_idx, row=64, col=8):\n",
    "    fig = plt.figure(figsize=(20,160))\n",
    "    fig.tight_layout()\n",
    "    for i in range(row):\n",
    "        for j in range(col):\n",
    "            ind = i*col + j\n",
    "            ax = fig.add_subplot(row,col,ind+1,xticks=[],yticks=[])\n",
    "            ax.imshow(k[img_idx,ind,:,:].detach().cpu()) \n",
    "            ax.text(3, 6, ind, fontdict={'weight': 'bold', 'size': 16}, color=\"y\" )\n",
    "    fig.suptitle(\"feature map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_generation(images, targets, X_FM, Y_FM):\n",
    "    global ratios\n",
    "    global anchor_scales\n",
    "    num_batch = len(images)\n",
    "    X_IMG, Y_IMG = images[0].shape[1:]\n",
    "    bbox_all = [item['boxes'] for item in targets]\n",
    "    labels_all = [item['labels'] for item in targets]\n",
    "    \n",
    "    #imgs_torch_all = torch.stack([item for item in images])\n",
    "    #if is_cuda:\n",
    "    #    imgs_torch_all = imgs_torch_all.cuda()   \n",
    "    #k = imgs_torch_all.clone()\n",
    "    #for m in req_features:\n",
    "    #    k = m(k)\n",
    "    #print(k.shape) \n",
    "    \n",
    "\n",
    "    sub_sampling_x = int(X_IMG/X_FM)\n",
    "    sub_sampling_y = int(Y_IMG/Y_FM)\n",
    "    #print(X_IMG, Y_IMG, X_FM, Y_FM, sub_sampling_x,sub_sampling_y)\n",
    "    anchor_base = np.zeros((len(ratios)*len(anchor_scales), 4), dtype=np.float32)\n",
    "    \n",
    "    ctr_x = np.arange(sub_sampling_x, (X_FM+1) * sub_sampling_x, sub_sampling_x)\n",
    "    ctr_y = np.arange(sub_sampling_y, (Y_FM+1) * sub_sampling_y, sub_sampling_y)\n",
    "    index = 0\n",
    "    ctr = np.zeros((len(ctr_y)*len(ctr_y),2),dtype=np.float32)\n",
    "    for x in range(len(ctr_x)):\n",
    "        for y in range(len(ctr_y)):\n",
    "            ctr[index, 1] = ctr_x[x] - 8\n",
    "            ctr[index, 0] = ctr_y[y] - 8\n",
    "            index +=1 \n",
    "            \n",
    "    anchors = np.zeros((X_FM * Y_FM * 9, 4))\n",
    "    index = 0\n",
    "    for ctr_y, ctr_x in ctr:\n",
    "        for i in range(len(ratios)):\n",
    "            for j in range(len(anchor_scales)):\n",
    "                h = sub_sampling_x * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "                w = sub_sampling_y * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "                anchors[index, 0] = ctr_y - h / 2.\n",
    "                anchors[index, 1] = ctr_x - w / 2.\n",
    "                anchors[index, 2] = ctr_y + h / 2.\n",
    "                anchors[index, 3] = ctr_x + w / 2.\n",
    "                index += 1\n",
    "    #print(anchors.shape)    \n",
    "    \n",
    "    index_inside = np.where(\n",
    "        (anchors[:, 0] >= 0) &\n",
    "        (anchors[:, 1] >= 0) &\n",
    "        (anchors[:, 2] <= Y_IMG) &\n",
    "        (anchors[:, 3] <= X_IMG)\n",
    "    )[0]\n",
    "    #print(index_inside.shape)\n",
    "    \n",
    "    label = np.empty((len(index_inside), ), dtype=np.int32)\n",
    "    label.fill(-1)\n",
    "    valid_anchors = anchors[index_inside]\n",
    "    #print(label.shape, valid_anchors.shape)\n",
    "    #print(valid_anchors[0])  \n",
    "    \n",
    "    ious_all = []\n",
    "    for bx in bbox_all:\n",
    "        ious = np.empty((len(label), bx.size()[0]), dtype=np.float32)\n",
    "        ious.fill(0)\n",
    "        for num1, i in enumerate(valid_anchors):\n",
    "            ya1, xa1, ya2, xa2 = i  \n",
    "            anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "            for num2, j in enumerate(bx):\n",
    "                yb1, xb1, yb2, xb2 = j\n",
    "                box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "                inter_x1 = max([xb1, xa1])\n",
    "                inter_y1 = max([yb1, ya1])\n",
    "                inter_x2 = min([xb2, xa2])\n",
    "                inter_y2 = min([yb2, ya2])\n",
    "                if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "                    iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "                    iou = iter_area / (anchor_area+ box_area - iter_area)            \n",
    "                else:\n",
    "                    iou = 0.\n",
    "                ious[num1, num2] = iou\n",
    "        ious_all.append(ious)\n",
    "        \n",
    "        \n",
    "    gt_argmax_ious_all = []\n",
    "    gt_max_ious_all = []\n",
    "    for ious_ in ious_all:\n",
    "        gt_argmax_ious = ious_.argmax(axis=0)\n",
    "        gt_max_ious = ious_[gt_argmax_ious, np.arange(ious_.shape[1])]\n",
    "        gt_argmax_ious_all.append(gt_argmax_ious)\n",
    "        gt_max_ious_all.append(gt_max_ious)\n",
    "    #print(gt_argmax_ious_all)  \n",
    "    #print(gt_max_ious_all)   \n",
    "    \n",
    "    argmax_ious_all = []\n",
    "    max_ious_all = []\n",
    "    for ious_ in ious_all:\n",
    "        argmax_ious = ious_.argmax(axis=1)\n",
    "        max_ious = ious_[np.arange(len(label)), argmax_ious]\n",
    "        argmax_ious_all.append(argmax_ious)\n",
    "        max_ious_all.append(max_ious)\n",
    "    #print(argmax_ious_all)    \n",
    "    #print(max_ious_all)  \n",
    "    \n",
    "    gt_argmax_ious_all = []\n",
    "    for gt_max_ious_, ious_ in zip(gt_max_ious_all, ious_all):\n",
    "        gt_argmax_ious = np.where(ious_ == gt_max_ious_)[0]\n",
    "        gt_argmax_ious_all.append(gt_argmax_ious)\n",
    "    #print(gt_argmax_ious_all)       \n",
    "\n",
    "    pos_iou_threshold  = 0.7\n",
    "    neg_iou_threshold = 0.3  \n",
    "    \n",
    "    label_all = []\n",
    "    for n in range(num_batch):\n",
    "        l = copy.deepcopy(label)\n",
    "        l[max_ious_all[n] < neg_iou_threshold] = 0\n",
    "        l[gt_argmax_ious_all[n]] = 1\n",
    "        l[max_ious_all[n] >= pos_iou_threshold] = 1\n",
    "        label_all.append(l)\n",
    "    #print (\"label_all 0 and 1: \", sum(label_all[0]), sum(label_all[1]))     \n",
    "    \n",
    "    pos_ratio = 0.5\n",
    "    n_sample = 256\n",
    "    n_pos = int(pos_ratio * n_sample)\n",
    "    #print(n_pos)    \n",
    "    \n",
    "    for n in range(num_batch):\n",
    "        #print(np.sum((label_all[n] == 1)))\n",
    "        pos_index = np.where(label_all[n] == 1)[0]\n",
    "        #print(pos_index)\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "            label_all[n][disable_index] = -1\n",
    "        #print(np.sum((label_all[n] == 1)))  \n",
    "\n",
    "        n_neg = n_sample - np.sum(label_all[n] == 1)\n",
    "        neg_index = np.where(label_all[n] == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
    "            label_all[n][disable_index] = -1\n",
    "        #print(np.sum((label_all[n] == 0)))   \n",
    "        \n",
    "        \n",
    "    max_iou_bbox_all = []\n",
    "    #print(bbox_all)\n",
    "    for n in range(num_batch):\n",
    "        max_iou_bbox_all.append(bbox_all[n][argmax_ious_all[n]])\n",
    "    #print(max_iou_bbox_all[0].shape, max_iou_bbox_all[0].shape)           \n",
    "        \n",
    "    #Anchor box\n",
    "    height = valid_anchors[:, 2] - valid_anchors[:, 0]\n",
    "    width = valid_anchors[:, 3] - valid_anchors[:, 1]\n",
    "    ctr_y = valid_anchors[:, 0] + 0.5 * height\n",
    "    ctr_x = valid_anchors[:, 1] + 0.5 * width\n",
    "    #Ground truth\n",
    "    base_height_all = []\n",
    "    base_width_all = []\n",
    "    base_ctr_y_all = []\n",
    "    base_ctr_x_all = []\n",
    "    for n in range(num_batch):\n",
    "        base_height = max_iou_bbox_all[n][:, 2] - max_iou_bbox_all[n][:, 0]\n",
    "        base_width = max_iou_bbox_all[n][:, 3] - max_iou_bbox_all[n][:, 1]\n",
    "        base_ctr_y = max_iou_bbox_all[n][:, 0] + 0.5 * base_height\n",
    "        base_ctr_x = max_iou_bbox_all[n][:, 1] + 0.5 * base_width\n",
    "        base_height_all.append(base_height)\n",
    "        base_width_all.append(base_width)\n",
    "        base_ctr_y_all.append(base_ctr_y)\n",
    "        base_ctr_x_all.append(base_ctr_x)\n",
    "\n",
    "    #print(width[2], base_width_all[0][2])        \n",
    "        \n",
    "    #Prevent devide by 0\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    #d_{} calculatrion\n",
    "    anchor_locs_all = []\n",
    "    for n in range(num_batch):\n",
    "        dy = (base_ctr_y_all[n].numpy() - ctr_y) / height\n",
    "        dx = (base_ctr_x_all[n].numpy() - ctr_x) / width\n",
    "        dh = np.log(base_height_all[n].numpy()/ height)\n",
    "        dw = np.log(base_width_all[n].numpy() / width)\n",
    "        anchor_locs_all.append(np.vstack((dy, dx,dh, dw)).transpose())\n",
    "    #print(anchor_locs_all[0][1], anchor_locs_all[0].shape)        \n",
    "        \n",
    "    anchor_labels_all = []\n",
    "    for n in range(num_batch):\n",
    "        anchor_labels = np.empty((len(anchors),), dtype=label_all[n].dtype)\n",
    "        anchor_labels.fill(-1)\n",
    "        anchor_labels[index_inside] = label_all[n]\n",
    "        anchor_labels_all.append(anchor_labels)\n",
    "    anchor_labels_all_merge = np.stack(anchor_labels_all, 0)    \n",
    "    #print(sum(anchor_labels_all[0]==1), anchor_labels_all[0][0:11])\n",
    "    #print(anchor_labels_all_merge.shape)\n",
    "    #print(sum(anchor_labels_all_merge[0]==1))    \n",
    "    \n",
    "    anchor_locations_all = []\n",
    "    for n in range(num_batch):\n",
    "        anchor_locations = np.empty((len(anchors), anchors.shape[1]), dtype=anchor_locs_all[n].dtype)\n",
    "        anchor_locations.fill(0)\n",
    "        anchor_locations[index_inside, :] = anchor_locs_all[n]\n",
    "        anchor_locations_all.append(anchor_locations)\n",
    "    #print(anchor_locations_all[0].shape)   \n",
    "    #print(type(anchor_locations_all[0]))\n",
    "    anchor_locations_all_merge = np.stack(anchor_locations_all, 0)\n",
    "    #print(anchor_locations_all_merge[0][0])\n",
    "    #print(anchor_locations_all[0][1500])    \n",
    "  \n",
    "    return anchor_locations_all_merge, anchor_labels_all_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_cuda= False\n",
    "if torch.cuda.is_available():\n",
    "    is_cuda = True\n",
    "print(is_cuda)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = img.resize(ISIZE)\n",
    "        img = np.array(img)\n",
    "        img = normalize(img)\n",
    "        img = img.transpose(2,0,1)\n",
    "        img = torch.as_tensor(img, dtype=torch.float32)\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.resize(ISIZE)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        #print('********************:  ', idx, img.shape)\n",
    "        #print('target: ', target['labels'])\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('../../Data/PennFudanPed', None)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=15, shuffle=True, num_workers=2, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True).cuda()\n",
    "if is_cuda:\n",
    "    model.cuda() \n",
    "model.eval()    \n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "fe = list(model.features)\n",
    "    \n",
    "req_features = []\n",
    "for j, i in enumerate(fe[0:30]):\n",
    "    req_features.append(i)\n",
    "#print (req_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs(req_features, model, optimizer, train_dl, val_dl, epochs=10,  rpn_lambda=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        sum_loss = 0\n",
    "        sum_loss_cls = 0\n",
    "        sum_loss_loc = 0\n",
    "        idx = 0\n",
    "        for images, targets in train_dl:\n",
    "            idx += 1\n",
    "            num_batch =  len(images) \n",
    "            print(\"######### %s epoch: %s\" %(idx, epoch))\n",
    "            imgs_torch_all = torch.stack([item for item in images])\n",
    "            if is_cuda:\n",
    "                imgs_torch_all = imgs_torch_all.cuda()   \n",
    "            k = imgs_torch_all.clone()\n",
    "            for m in req_features:\n",
    "                k = m(k)\n",
    "            #print(\"##############   feature map shape: \", k.shape) \n",
    "            X_FM, Y_FM = k.shape[2:]\n",
    "            #print(\"##############  X_FM, Y_FM: \",  X_FM, Y_FM )\n",
    "            anchor_locations_all_merge, anchor_labels_all_merge = bbox_generation(images, targets, X_FM, Y_FM)\n",
    "            \n",
    "            pred_anchor_locs, pred_cls_scores, objectness_score = model(k)\n",
    "\n",
    "            rpn_loc_all = pred_anchor_locs.view(1,-1,4).squeeze(0)\n",
    "            rpn_score_all = pred_cls_scores.view(1,-1,2).squeeze(0)\n",
    "            #print(rpn_loc_all.shape, rpn_score_all.shape)\n",
    "            gt_rpn_loc_all = torch.from_numpy(anchor_locations_all_merge.astype(np.float32)).view(1,-1,4).squeeze(0)\n",
    "            gt_rpn_score_all = torch.from_numpy(anchor_labels_all_merge.astype(np.float32)).view(1,-1).squeeze(0)\n",
    "            if is_cuda:\n",
    "                gt_rpn_loc_all = gt_rpn_loc_all.cuda()\n",
    "                gt_rpn_score_all = gt_rpn_score_all.cuda()\n",
    "            #print(rpn_loc_all.shape, rpn_score_all.shape, gt_rpn_loc_all.shape, gt_rpn_score_all.shape)\n",
    "            \n",
    "            #print(rpn_score_all.shape, gt_rpn_score_all.shape)\n",
    "            rpn_cls_loss_all = F.cross_entropy(rpn_score_all, gt_rpn_score_all.long(), ignore_index = -1)\n",
    "            #print(rpn_cls_loss_all)\n",
    "            \n",
    "            pos_all = gt_rpn_score_all > 0\n",
    "            mask_all = pos_all.unsqueeze(1).expand_as(rpn_loc_all)\n",
    "            #print(pos_all.shape, pos_all.unsqueeze(1).shape, rpn_loc_all.shape,  pos_all.unsqueeze(1).expand_as(rpn_loc_all).shape) \n",
    "            num_valid_loc_all = 0\n",
    "            for i, j in enumerate(pos_all):\n",
    "                if j.item() == True:\n",
    "                    num_valid_loc_all += 1\n",
    "            #print (num_valid_loc_all)              \n",
    "            \n",
    "            \n",
    "            mask_loc_preds_all = rpn_loc_all[mask_all].view(-1, 4)\n",
    "            #print(mask_loc_preds_all.shape)\n",
    "            mask_loc_targets_all = gt_rpn_loc_all[mask_all].view(-1, 4)\n",
    "            #print(mask_loc_preds_all.dtype, mask_loc_targets_all.dtype,gt_rpn_loc_all.dtype )        \n",
    "            \n",
    "            x_all = torch.abs(mask_loc_targets_all - mask_loc_preds_all)\n",
    "            rpn_loc_loss_all = ((x_all < 1).float() * 0.5 * x_all**2) + ((x_all >= 1).float() * (x_all-0.5))\n",
    "            #print(rpn_loc_loss_all.sum())            \n",
    "            \n",
    "            N_reg_all = (gt_rpn_score_all >0).float().sum()\n",
    "            rpn_loc_loss_all = rpn_loc_loss_all.sum() / N_reg_all\n",
    "            rpn_loss_all = rpn_cls_loss_all + (rpn_lambda * rpn_loc_loss_all)\n",
    "            #print(rpn_cls_loss_all, rpn_loc_loss_all, rpn_loss_all)   \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            rpn_loss_all.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total += num_batch\n",
    "            sum_loss += rpn_loss_all.item()\n",
    "            sum_loss_cls += rpn_cls_loss_all.item()\n",
    "            sum_loss_loc += (rpn_lambda * rpn_loc_loss_all).item()\n",
    "        train_loss = sum_loss/total\n",
    "        train_loss_cls = sum_loss_cls/total\n",
    "        train_loss_loc = sum_loss_loc/total\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), './rpn_%s.pth'%i)\n",
    "        print(\"train_loss %.3f cls_loss %.3f loc_loss %.3f\" % (train_loss, train_loss_cls, train_loss_loc))\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=512, mid_channels=512, n_anchor=9):\n",
    "        super(RPN, self).__init__()   \n",
    "        self.mid_channels = mid_channels\n",
    "        self.in_channels = in_channels # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "        self.n_anchor = n_anchor # Number of anchors at each location\n",
    "        self.conv1 = nn.Conv2d(self.in_channels, self.mid_channels, 3, 1, 1)\n",
    "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0)\n",
    "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0) \n",
    "        \n",
    "        # conv sliding layer\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        # Regression layer\n",
    "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
    "        self.reg_layer.bias.data.zero_()\n",
    "        # classification layer\n",
    "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
    "        self.cls_layer.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, k):\n",
    "        bat_num = k.shape[0]\n",
    "        x = self.conv1(k)\n",
    "        pred_anchor_locs = self.reg_layer(x)\n",
    "        pred_cls_scores = self.cls_layer(x)\n",
    "        \n",
    "        pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(bat_num, -1, 4)\n",
    "        pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "        objectness_score = pred_cls_scores.view(bat_num, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(bat_num, -1)\n",
    "        pred_cls_scores  = pred_cls_scores.view(bat_num, -1, 2)\n",
    "        \n",
    "        return pred_anchor_locs, pred_cls_scores, objectness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RPN()\n",
    "if is_cuda:\n",
    "    model = model.cuda()\n",
    "parameters = model.parameters()\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### 1 epoch: 0\n",
      "######### 2 epoch: 0\n",
      "######### 3 epoch: 0\n",
      "######### 4 epoch: 0\n",
      "######### 5 epoch: 0\n",
      "######### 6 epoch: 0\n",
      "######### 7 epoch: 0\n",
      "######### 8 epoch: 0\n",
      "######### 9 epoch: 0\n",
      "######### 10 epoch: 0\n",
      "######### 11 epoch: 0\n",
      "######### 12 epoch: 0\n",
      "train_loss 6.326 cls_loss 0.140 loc_loss 6.186\n",
      "######### 1 epoch: 1\n",
      "######### 2 epoch: 1\n",
      "######### 3 epoch: 1\n",
      "######### 4 epoch: 1\n",
      "######### 5 epoch: 1\n",
      "######### 6 epoch: 1\n",
      "######### 7 epoch: 1\n",
      "######### 8 epoch: 1\n",
      "######### 9 epoch: 1\n",
      "######### 10 epoch: 1\n",
      "######### 11 epoch: 1\n",
      "######### 12 epoch: 1\n",
      "train_loss 1.968 cls_loss 0.049 loc_loss 1.919\n",
      "######### 1 epoch: 2\n",
      "######### 2 epoch: 2\n",
      "######### 3 epoch: 2\n",
      "######### 4 epoch: 2\n",
      "######### 5 epoch: 2\n",
      "######### 6 epoch: 2\n",
      "######### 7 epoch: 2\n",
      "######### 8 epoch: 2\n",
      "######### 9 epoch: 2\n",
      "######### 10 epoch: 2\n",
      "######### 11 epoch: 2\n",
      "######### 12 epoch: 2\n",
      "train_loss 1.701 cls_loss 0.036 loc_loss 1.665\n",
      "######### 1 epoch: 3\n",
      "######### 2 epoch: 3\n",
      "######### 3 epoch: 3\n",
      "######### 4 epoch: 3\n",
      "######### 5 epoch: 3\n",
      "######### 6 epoch: 3\n",
      "######### 7 epoch: 3\n",
      "######### 8 epoch: 3\n",
      "######### 9 epoch: 3\n",
      "######### 10 epoch: 3\n",
      "######### 11 epoch: 3\n",
      "######### 12 epoch: 3\n",
      "train_loss 1.219 cls_loss 0.022 loc_loss 1.197\n",
      "######### 1 epoch: 4\n",
      "######### 2 epoch: 4\n",
      "######### 3 epoch: 4\n",
      "######### 4 epoch: 4\n",
      "######### 5 epoch: 4\n",
      "######### 6 epoch: 4\n",
      "######### 7 epoch: 4\n",
      "######### 8 epoch: 4\n",
      "######### 9 epoch: 4\n",
      "######### 10 epoch: 4\n",
      "######### 11 epoch: 4\n",
      "######### 12 epoch: 4\n",
      "train_loss 0.953 cls_loss 0.017 loc_loss 0.937\n",
      "######### 1 epoch: 5\n",
      "######### 2 epoch: 5\n",
      "######### 3 epoch: 5\n",
      "######### 4 epoch: 5\n",
      "######### 5 epoch: 5\n",
      "######### 6 epoch: 5\n",
      "######### 7 epoch: 5\n",
      "######### 8 epoch: 5\n",
      "######### 9 epoch: 5\n",
      "######### 10 epoch: 5\n",
      "######### 11 epoch: 5\n",
      "######### 12 epoch: 5\n",
      "train_loss 0.557 cls_loss 0.014 loc_loss 0.543\n",
      "######### 1 epoch: 6\n",
      "######### 2 epoch: 6\n",
      "######### 3 epoch: 6\n",
      "######### 4 epoch: 6\n",
      "######### 5 epoch: 6\n",
      "######### 6 epoch: 6\n",
      "######### 7 epoch: 6\n",
      "######### 8 epoch: 6\n",
      "######### 9 epoch: 6\n",
      "######### 10 epoch: 6\n",
      "######### 11 epoch: 6\n",
      "######### 12 epoch: 6\n",
      "train_loss 0.824 cls_loss 0.011 loc_loss 0.813\n",
      "######### 1 epoch: 7\n",
      "######### 2 epoch: 7\n",
      "######### 3 epoch: 7\n",
      "######### 4 epoch: 7\n",
      "######### 5 epoch: 7\n",
      "######### 6 epoch: 7\n",
      "######### 7 epoch: 7\n",
      "######### 8 epoch: 7\n",
      "######### 9 epoch: 7\n",
      "######### 10 epoch: 7\n",
      "######### 11 epoch: 7\n",
      "######### 12 epoch: 7\n",
      "train_loss 0.532 cls_loss 0.009 loc_loss 0.523\n",
      "######### 1 epoch: 8\n",
      "######### 2 epoch: 8\n",
      "######### 3 epoch: 8\n",
      "######### 4 epoch: 8\n",
      "######### 5 epoch: 8\n",
      "######### 6 epoch: 8\n",
      "######### 7 epoch: 8\n",
      "######### 8 epoch: 8\n",
      "######### 9 epoch: 8\n",
      "######### 10 epoch: 8\n",
      "######### 11 epoch: 8\n",
      "######### 12 epoch: 8\n",
      "train_loss 0.553 cls_loss 0.011 loc_loss 0.542\n",
      "######### 1 epoch: 9\n",
      "######### 2 epoch: 9\n",
      "######### 3 epoch: 9\n",
      "######### 4 epoch: 9\n",
      "######### 5 epoch: 9\n",
      "######### 6 epoch: 9\n",
      "######### 7 epoch: 9\n",
      "######### 8 epoch: 9\n",
      "######### 9 epoch: 9\n",
      "######### 10 epoch: 9\n",
      "######### 11 epoch: 9\n",
      "######### 12 epoch: 9\n",
      "train_loss 0.513 cls_loss 0.008 loc_loss 0.504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epocs(req_features, model, optimizer, data_loader, None, epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
